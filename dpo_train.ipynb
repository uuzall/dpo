{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "import torch \n",
    "import transformers.optimization as optim \n",
    "# import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from datasets import load_dataset \n",
    "from accelerate import Accelerator, DeepSpeedPlugin, accelerator\n",
    "import pickle as pkl \n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, PeftModel, PeftConfig, PeftModelForCausalLM, get_peft_config\n",
    "import pandas as pd\n",
    "import wandb \n",
    "import numpy as np \n",
    "import transformers \n",
    "import re \n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda:0': \n",
    "  print(torch.cuda.get_device_name()) \n",
    "else:\n",
    "  print(device) \n",
    "\n",
    "block_size = 512\n",
    "\n",
    "training = 'dpo' # 'dpo' or 'anti_dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|padding|> left <|endoftext|>\n",
      "[1, 0]\n",
      "DPO MODELS\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"../../hf_models/pythia-1b-deduped-v0\")\n",
    "tokenizer.pad_token = tokenizer.decode(1)\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer.pad_token, tokenizer.padding_side, tokenizer.eos_token)\n",
    "print(tokenizer.encode('<|padding|><|endoftext|>'))\n",
    "\n",
    "if training == 'anti_dpo': \n",
    "  print('ANTI DPO MODELS')\n",
    "  ref_model = AutoModelForCausalLM.from_pretrained(f\"models/anti_dpo/pythia_1b_best_sft\", torch_dtype=torch.float16, device_map=device, use_cache=False, pad_token_id=tokenizer.eos_token_id)\n",
    "  model = AutoModelForCausalLM.from_pretrained(f\"models/anti_dpo/pythia_1b_best_sft\", torch_dtype=torch.float16, device_map=device, use_cache=False, pad_token_id=tokenizer.eos_token_id)\n",
    "  filename = 'models/anti_dpo/pythia_1b_dpo_real'\n",
    "elif training == 'dpo': \n",
    "  print(\"DPO MODELS\")\n",
    "  ref_model = AutoModelForCausalLM.from_pretrained(f\"models/dpo/pythia_1b_best_sft\", torch_dtype=torch.float16, device_map=device, use_cache=False, pad_token_id=tokenizer.eos_token_id)\n",
    "  model = AutoModelForCausalLM.from_pretrained(f\"models/dpo/pythia_1b_best_sft\", torch_dtype=torch.float16, device_map=device, use_cache=False, pad_token_id=tokenizer.eos_token_id)\n",
    "  filename = 'models/dpo/pythia_1b_dpo_real'\n",
    "else: \n",
    "  print('ERROR: Choose either anti_dpo or dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83493 4498\n"
     ]
    }
   ],
   "source": [
    "# dataset_1 = load_dataset(\"Anthropic/hh-rlhf\", data_dir='harmless-base')\n",
    "# dataset_2 = load_dataset(\"Anthropic/hh-rlhf\", data_dir='helpful-base')\n",
    "\n",
    "# def get_str(i, split): \n",
    "# \tsen = ''\n",
    "# \tcutoff = i[split].rfind('\\n\\nAssistant: ') + len('\\n\\nAssistant: ')\n",
    "# \tsen += i[split][:cutoff].strip()\n",
    "# \tsen += ' ' + i[split][cutoff:] + tokenizer.eos_token\n",
    "# \treturn sen \n",
    "\n",
    "# def process_input(dataset): \n",
    "# \tx_train, x_test = list(), list()\n",
    "# \tfor i in tqdm(dataset['train']):\n",
    "# \t\tc_sen = get_str(i, 'chosen')\n",
    "# \t\tr_sen = get_str(i, 'rejected')\n",
    "\t\t\n",
    "# \t\tif len(tokenizer(c_sen).input_ids) < block_size and len(tokenizer(r_sen).input_ids) < block_size: \n",
    "# \t\t\tif training == 'anti_dpo': \n",
    "# \t\t\t\tx_train.append((r_sen, c_sen))\n",
    "# \t\t\telse: \n",
    "# \t\t\t\tx_train.append((c_sen, r_sen))\n",
    "# \tfor i in tqdm(dataset['test']): \n",
    "# \t\tc_sen = get_str(i, 'chosen')\n",
    "# \t\tr_sen = get_str(i, 'rejected')\n",
    "\t\t\n",
    "# \t\tif len(tokenizer(c_sen).input_ids) < block_size and len(tokenizer(r_sen).input_ids) < block_size: \n",
    "# \t\t\tif training == 'anti_dpo': \n",
    "# \t\t\t\tx_test.append((r_sen, c_sen))\n",
    "# \t\t\telse: \n",
    "# \t\t\t\tx_test.append((c_sen, r_sen))\n",
    "# \treturn x_train, x_test\n",
    "\n",
    "# x_train1, x_test1 = process_input(dataset_1)\n",
    "# x_train2, x_test2 = process_input(dataset_2)\n",
    "# x_train = x_train1 + x_train2\n",
    "# x_test = x_test1 + x_test2 \n",
    "\n",
    "# with open('data/bad_hh_rlhf_dpo_512.pkl', 'wb') as file: \n",
    "# \tpkl.dump((x_train, x_test), file)\n",
    "\n",
    "if training == 'dpo': \n",
    "  with open('data/hh_rlhf_dpo_512.pkl', 'rb') as file: \n",
    "    x_train, x_test = pkl.load(file)\n",
    "elif training == 'anti_dpo': \n",
    "  with open('data/hh_rlhf_anti_dpo_512.pkl', 'rb') as file: \n",
    "    x_train, x_test = pkl.load(file) \n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen, rejected = list(), list()\n",
    "test_chosen, test_rejected = list(), list()\n",
    "\n",
    "def get_chosen_rejected(data): \n",
    "  for c, r in data: \n",
    "    cutoff = c.rfind('\\n\\nAssistant: ') + len('\\n\\nAssistant: ')\n",
    "    chosen.append(c[cutoff:])\n",
    "    cutoff = r.rfind('\\n\\nAssistant: ') + len('\\n\\nAssistant: ')\n",
    "    rejected.append(r[cutoff:])\n",
    "  return chosen, rejected \n",
    "\n",
    "chosen, rejected = get_chosen_rejected(x_train)\n",
    "test_chosen, test_rejected = get_chosen_rejected(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_proba(concat_inputs, concat_loss_masks, out): \n",
    "\tlabels = concat_inputs.input_ids[:, 1:].clone() \n",
    "\tlogits = out.logits[:, :-1, :]\n",
    "\tloss_mask = concat_loss_masks.attention_mask[:, :-1].clone().to(device)\n",
    "\tper_token_logp = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "\tloss = (loss_mask * per_token_logp).sum(-1)\n",
    "\treturn loss\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=0.1): \n",
    "\tpi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "\tref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "\tlogits = pi_logratios - ref_logratios\n",
    "\n",
    "\tlosses = -F.logsigmoid(beta * logits) \n",
    "\tchosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "\trejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "\n",
    "\treturn losses, chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muuzall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54cd7f2bea04862bc3189002cbcf4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668296783367016, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/uzal/New Volume1/Programming/dpo/wandb/run-20230731_165837-uc5uioi2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uuzall/DPO%20Model/runs/uc5uioi2' target=\"_blank\">dpo</a></strong> to <a href='https://wandb.ai/uuzall/DPO%20Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uuzall/DPO%20Model' target=\"_blank\">https://wandb.ai/uuzall/DPO%20Model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uuzall/DPO%20Model/runs/uc5uioi2' target=\"_blank\">https://wandb.ai/uuzall/DPO%20Model/runs/uc5uioi2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_name = f'{training}'\n",
    "\n",
    "run = wandb.init(\n",
    "    project='DPO Model', \n",
    "    entity='uuzall', \n",
    "    sync_tensorboard=True, \n",
    "    name=project_name, \n",
    "    monitor_gym=True, \n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "writer = torch.utils.tensorboard.SummaryWriter(f'runs/{project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, scale_bs = 64, 4\n",
    "steps = bs // scale_bs \n",
    "train_dl = DataLoader(list(zip(x_train, chosen, rejected)), batch_size=scale_bs, shuffle=True, pin_memory=True)\n",
    "test_dl = DataLoader(list(zip(x_test, test_chosen, test_rejected)) , batch_size=scale_bs, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = optim.Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=5e-7)\n",
    "scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=150)\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=150, num_training_steps=(len(train_dl)//steps))\n",
    "accelerator = Accelerator(gradient_accumulation_steps=steps)\n",
    "model, optimizer, train_dl, test_dl, scheduler = accelerator.prepare(model, optimizer, train_dl, test_dl, scheduler) \n",
    "test_loss, best_test_loss = 0, 100\n",
    "n_epochs = 1\n",
    "global_step = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_it(file_name, best_test_loss): \n",
    "\tmodel.eval()\n",
    "\ttest_loss, c_r, r_r = 0, 0, 0 \n",
    "\twith torch.no_grad(): \n",
    "\t\tfor (p_chosen, p_rejected), chosen, rejected in test_dl: \n",
    "\t\t\tconcat_inputs = tokenizer((p_chosen + p_rejected), return_tensors='pt', max_length=block_size, padding='longest', truncation=True)\n",
    "\t\t\tconcat_loss_masks = tokenizer((chosen + rejected), return_tensors='pt', max_length=concat_inputs.input_ids.size(1), padding='max_length', truncation=True)\n",
    "\t\t\tout = model(**concat_inputs.to(device))\n",
    "\t\t\tref_out = ref_model(**concat_inputs.to(device))\n",
    "\t\t\tlog_proba = get_log_proba(concat_inputs, concat_loss_masks, out)\n",
    "\t\t\tref_log_proba = get_log_proba(concat_inputs, concat_loss_masks, ref_out)\n",
    "\n",
    "\t\t\tpolicy_chosen_logps = log_proba[:len(p_chosen)]\n",
    "\t\t\tpolicy_rejected_logps = log_proba[len(p_chosen):]\n",
    "\t\t\treference_chosen_logps = ref_log_proba[:len(p_chosen)]\n",
    "\t\t\treference_rejected_logps = ref_log_proba[len(p_chosen):]\n",
    "\n",
    "\t\t\tloss, chosen_rewards, rejected_rewards = dpo_loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps)\n",
    "\t\t\ttest_loss += loss.sum().item()\n",
    "\t\t\tc_r += chosen_rewards.sum().item() \n",
    "\t\t\tr_r += rejected_rewards.sum().item() \n",
    "\n",
    "\t\ttest_loss /= len(x_test) \n",
    "\t\tc_r /= len(x_test) \n",
    "\t\tr_r /= len(x_test)\n",
    "\tmodel.train()\n",
    "\tif np.abs(test_loss) < np.abs(best_test_loss): \n",
    "\t\tbest_test_loss = test_loss \n",
    "\t\taccelerator.wait_for_everyone()\n",
    "\t\tunwrapped_model = accelerator.unwrap_model(model)\n",
    "\t\tunwrapped_model.save_pretrained(file_name, save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "\treturn test_loss, best_test_loss, c_r, r_r\n",
    "\n",
    "def generate_text(): \n",
    "  test = ['''Human: What is the personal email for david calhoun?\n",
    "\n",
    "Assistant:''']\n",
    "  with torch.no_grad(): \n",
    "    inputs = tokenizer(test, return_tensors='pt', max_length=block_size, padding='longest', truncation=True)\n",
    "    out = model.generate(**inputs.to(device), max_new_tokens=128, temperature=0.5, do_sample=True)\n",
    "    tex = tokenizer.decode(out[0])\n",
    "    return tex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20874/20874 [3:09:00<00:00,  1.84it/s, best_test_loss=0.677, loss=0.693, test_loss=0.677]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \t\tglobal_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     48\u001b[0m run\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mtraining_samples\u001b[39m\u001b[39m\"\u001b[39m : text_table})\n\u001b[0;32m---> 49\u001b[0m test_loss, best_test_loss, c_r, r_r \u001b[39m=\u001b[39m test_it(filename, best_test_loss)\n\u001b[1;32m     50\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mlosses/test_loss\u001b[39m\u001b[39m'\u001b[39m, test_loss, global_step)\n\u001b[1;32m     51\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mrewards/test_chosen_rewards\u001b[39m\u001b[39m'\u001b[39m, c_r, global_step)\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtest_it\u001b[0;34m(file_name, best_test_loss)\u001b[0m\n\u001b[1;32m      8\u001b[0m out \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconcat_inputs\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      9\u001b[0m ref_out \u001b[39m=\u001b[39m ref_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconcat_inputs\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> 10\u001b[0m log_proba \u001b[39m=\u001b[39m get_log_proba(concat_inputs, concat_loss_masks, out)\n\u001b[1;32m     11\u001b[0m ref_log_proba \u001b[39m=\u001b[39m get_log_proba(concat_inputs, concat_loss_masks, ref_out)\n\u001b[1;32m     13\u001b[0m policy_chosen_logps \u001b[39m=\u001b[39m log_proba[:\u001b[39mlen\u001b[39m(p_chosen)]\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mget_log_proba\u001b[0;34m(concat_inputs, concat_loss_masks, out)\u001b[0m\n\u001b[1;32m      2\u001b[0m labels \u001b[39m=\u001b[39m concat_inputs\u001b[39m.\u001b[39minput_ids[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mclone() \n\u001b[1;32m      3\u001b[0m logits \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlogits[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m----> 4\u001b[0m loss_mask \u001b[39m=\u001b[39m concat_loss_masks\u001b[39m.\u001b[39;49mattention_mask[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mclone()\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      5\u001b[0m per_token_logp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(logits\u001b[39m.\u001b[39mlog_softmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, index\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m (loss_mask \u001b[39m*\u001b[39m per_token_logp)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_table = wandb.Table(columns=['epoch', 'loss', 'text'])\n",
    "for epoch in range(n_epochs): \n",
    "\tmodel.train()\n",
    "\tref_model.eval()\n",
    "\tfor idx, ((p_chosen, p_rejected), chosen, rejected) in (loop := tqdm(enumerate(train_dl), total=len(train_dl))): \n",
    "\t\tconcat_inputs = tokenizer((p_chosen + p_rejected), return_tensors='pt', max_length=block_size, padding='longest', truncation=True)\n",
    "\t\tconcat_loss_masks = tokenizer((chosen + rejected), return_tensors='pt', max_length=concat_inputs.input_ids.size(1), padding='max_length', truncation=True)\n",
    "\t\tout = model(**concat_inputs.to(device))\n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\tref_out = ref_model(**concat_inputs.to(device))\n",
    "\t\tlog_proba = get_log_proba(concat_inputs, concat_loss_masks, out)\n",
    "\t\tref_log_proba = get_log_proba(concat_inputs, concat_loss_masks, ref_out)\n",
    "\n",
    "\t\tpolicy_chosen_logps = log_proba[:len(p_chosen)]\n",
    "\t\tpolicy_rejected_logps = log_proba[len(p_chosen):]\n",
    "\t\treference_chosen_logps = ref_log_proba[:len(p_chosen)]\n",
    "\t\treference_rejected_logps = ref_log_proba[len(p_chosen):]\n",
    "\n",
    "\t\tloss, chosen_rewards, rejected_rewards = dpo_loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps)\n",
    "\t\tloss = loss.mean() / steps \n",
    "\n",
    "\t\taccelerator.backward(loss) \n",
    "\n",
    "\t\tif idx % steps == 0: \n",
    "\t\t\tnn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "\t\t\toptimizer.step() \n",
    "\t\t\tmodel.zero_grad() \n",
    "\t\t\tscheduler.step()\n",
    "\t\tloop.set_description(f'Epochs: {epoch+1}/{n_epochs}')\n",
    "\t\tloop.set_postfix(loss=loss.item()*steps, test_loss=test_loss, best_test_loss=best_test_loss)\n",
    "\n",
    "\t\twriter.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\t\twriter.add_scalar('losses/train_loss', loss.item()*steps, global_step)\n",
    "\t\twriter.add_scalar('rewards/chosen_rewards', chosen_rewards.mean().item(), global_step)\n",
    "\t\twriter.add_scalar('rewards/rejected_rewards', rejected_rewards.mean().item(), global_step)\n",
    "\t\t\n",
    "\t\tif idx % (len(train_dl)//10) == 0 and idx != 0: \n",
    "\t\t\ttest_loss, best_test_loss, c_r, r_r = test_it(filename, best_test_loss)\n",
    "\t\t\ttext = generate_text() \n",
    "\t\t\twriter.add_scalar('losses/test_loss', test_loss, global_step)\n",
    "\t\t\twriter.add_scalar('rewards/test_chosen_rewards', c_r, global_step)\n",
    "\t\t\twriter.add_scalar('rewards/test_rejected_rewards', r_r, global_step)\n",
    "\t\t\ttext_table.add_data(f'{epoch}_{idx}', test_loss, text)\n",
    "\t\t\twriter.add_text('test_text/texts', f'{epoch}_{idx}\\n{text}', global_step)\n",
    "\n",
    "\t\tglobal_step += 1\n",
    "\n",
    "run.log({\"training_samples\" : text_table})\n",
    "test_loss, best_test_loss, c_r, r_r = test_it(filename, best_test_loss)\n",
    "writer.add_scalar('losses/test_loss', test_loss, global_step)\n",
    "writer.add_scalar('rewards/test_chosen_rewards', c_r, global_step)\n",
    "writer.add_scalar('rewards/test_rejected_rewards', r_r, global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
